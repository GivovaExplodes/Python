# -*- coding: utf-8 -*-
"""lab5.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1HfxgOEcsZYxJltblYOJZwUe41wuzgApp
"""

import pandas as pd
from sklearn.pipeline import Pipeline
from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.ensemble import RandomForestRegressor
from sklearn.linear_model import LinearRegression
from sklearn.svm import SVR
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score

from sklearn.manifold import TSNE
from sklearn.decomposition import FastICA

df = pd.read_csv("./insurance.csv")
df.head()

df.tail()

df['region'].value_counts()

df.isna().sum()

df.describe()

X = df.drop('insurance_cost', axis=1)
y = df['insurance_cost']
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

numeric_features = ['age', 'bmi', 'children']
categorical_features = ['gender', 'smoker', 'region']

numeric_transformer = Pipeline(steps=[
    ('scaler', StandardScaler())
])

categorical_transformer = Pipeline(steps=[
    ('onehot', OneHotEncoder())
])

preprocessor = ColumnTransformer(
    transformers=[
        ('num', numeric_transformer, numeric_features),
        ('cat', categorical_transformer, categorical_features)
    ])

regressors = {
    'Random Forest': (RandomForestRegressor(), {'regressor__n_estimators': [10, 50, 100]}),
    'Linear Regression': (LinearRegression(), {}),
    'SVR': (SVR(), {'regressor__kernel': ['linear', 'rbf'], 'regressor__C': [0.1, 1, 10]})
}

results = {}
for name, (regressor, params) in regressors.items():
    pipe = Pipeline(steps=[('preprocessor', preprocessor), ('regressor', regressor)])
    grid_search = GridSearchCV(pipe, params, cv=5, n_jobs=-1)
    grid_search.fit(X_train, y_train)
    best_model = grid_search.best_estimator_
    y_pred = best_model.predict(X_test)
    mse = mean_squared_error(y_test, y_pred)
    mae = mean_absolute_error(y_test, y_pred)
    r2 = r2_score(y_test, y_pred)
    results[name] = {'best_model': best_model, 'mse': mse, 'mae': mae, 'r2': r2}

results_df = pd.DataFrame(results).T
print(results_df)

"""Overall, Random Forest seems to be the most suitable regression algorithm for predicting insurance costs in this dataset, as it provides the most accurate predictions. Its ensemble nature allows it to handle complex relationships between features and the target variable effectively. Linear Regression, while simpler, can still provide reasonable results but may struggle with capturing non-linear relationships present in the data. SVR, although capable of handling non-linear data, didn't exhibit superior performance compared to Random Forest and Linear Regression in this specific scenario."""

regressors = {
    'Random Forest': (RandomForestRegressor(), {'regressor__n_estimators': [10, 50, 100]}),
    'Linear Regression': (LinearRegression(), {}),
    'SVR': (SVR(), {'regressor__kernel': ['linear', 'rbf'], 'regressor__C': [0.1, 1, 10]})
}

dimensionality_reduction_methods = {
    't-SNE': TSNE(),
    'ICA': FastICA()
}

results = {}
for name, (regressor, params) in regressors.items():
    for dr_name, dr_method in dimensionality_reduction_methods.items():
        if dr_name == 't-SNE':
            dr_method = 'passthrough'
        pipe = Pipeline(steps=[('preprocessor', preprocessor), ('reduce_dim', dr_method), ('regressor', regressor)])
        grid_search = GridSearchCV(pipe, params, cv=5, n_jobs=-1, error_score='raise')
        try:
            grid_search.fit(X_train, y_train)
            best_model = grid_search.best_estimator_
            y_pred = best_model.predict(X_test)
            mse = mean_squared_error(y_test, y_pred)
            mae = mean_absolute_error(y_test, y_pred)
            r2 = r2_score(y_test, y_pred)
            results[f'{name} with {dr_name}'] = {'best_model': best_model, 'mse': mse, 'mae': mae, 'r2': r2}
        except Exception as e:
            print(f"An error occurred for {name} with {dr_name}: {str(e)}")

results_df = pd.DataFrame(results).T
print(results_df)

"""Random Forest regression consistently outperformed Linear Regression and Support Vector Regression (SVR) across all dimensionality reduction techniques. It showcased lower mean squared error (MSE), mean absolute error (MAE), and higher R2 scores, indicating its superiority in capturing the relationships within the data.

The choice of dimensionality reduction method influenced model performance. t-SNE yielded better results compared to Independent Component Analysis (ICA) across all regression algorithms. This suggests that t-SNE was more effective in capturing the underlying structure of the data while reducing dimensionality, leading to improved model performance.

SVR performed poorly in this experiment, exhibiting significantly higher MSE and MAE values and negative R2 scores. This indicates that SVR struggled to effectively capture the relationships within the data and generalize to unseen samples. Further investigation and fine-tuning of SVR parameters may be necessary to improve its performance.

Based on these findings, employing Random Forest regression with t-SNE for dimensionality reduction would be the recommended approach for predicting insurance costs in this dataset. This combination consistently demonstrated the best performance metrics, providing more accurate predictions compared to other models and dimensionality reduction techniques.
"""